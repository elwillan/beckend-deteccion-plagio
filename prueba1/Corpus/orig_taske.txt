En matemáticas y ciencias de la computación, la programación dinámica es un método para resolver problemas que exhiben las propiedades de subproblemas superpuestos y una subestructura óptima (descrita a continuación). El método lleva mucho menos tiempo que los métodos ingenuos.
El término fue utilizado originalmente en la década de 1940 por Richard Bellman para describir el proceso de resolución de problemas donde uno necesita encontrar las mejores decisiones una tras otra. Para 1953, había refinado esto al significado moderno. El campo fue fundado como un análisis de sistemas y un tema de ingeniería reconocido por el IEEE. La contribución de Bellman se recuerda en el nombre de la ecuación de Bellman, un resultado central de la programación dinámica que reafirma un problema de optimización en forma recursiva.
La palabra "programación" en "programación dinámica" no tiene ninguna conexión particular con la programación de computadoras, y en su lugar proviene del término "programación matemática", un sinónimo de optimización. Por lo tanto, el "programa" es el plan óptimo para la acción que se produce. Por ejemplo, un cronograma finalizado de eventos en una exposición a veces se llama programa. La programación, en este sentido, significa encontrar un plan de acción aceptable, un algoritmo.
La subestructura óptima significa que se pueden utilizar soluciones óptimas de subproblemas para encontrar las soluciones óptimas del problema general. Por ejemplo, la ruta más corta a una meta desde un vértice en un gráfico se puede encontrar calculando primero la ruta más corta a la meta desde todos los vértices adyacentes, y luego usando esto para elegir la mejor ruta general, como se muestra en la Figura 1. En En general, podemos resolver un problema con una subestructura óptima utilizando un proceso de tres pasos:
   1. Divida el problema en subproblemas más pequeños.
   2. Resuelva estos problemas de manera óptima utilizando este proceso de tres pasos de forma recursiva.
   3. Use estas soluciones óptimas para construir una solución óptima para el problema original.
Los subproblemas se resuelven dividiéndolos en subproblemas, y así sucesivamente, hasta que lleguemos a un caso simple que se pueda resolver en tiempo constante.
Figura 2. El gráfico de subproblema para la secuencia de Fibonacci. Que no es un árbol sino un DAG indica subproblemas superpuestos.
Decir que un problema tiene subproblemas superpuestos es decir que se usan los mismos subproblemas para resolver muchos problemas diferentes. Por ejemplo, en la secuencia de Fibonacci, F3 = F1 + F2 y F4 = F2 + F3: calcular cada número implica calcular F2. Debido a que se necesitan F3 y F4 para calcular F5, un enfoque ingenuo para calcular F5 puede terminar calculando F2 dos veces o más. Esto se aplica siempre que haya subproblemas superpuestos: un enfoque ingenuo puede perder el tiempo recalculando soluciones óptimas a los subproblemas que ya ha resuelto.
Para evitar esto, guardamos las soluciones a los problemas que ya hemos resuelto. Luego, si necesitamos resolver el mismo problema más tarde, podemos recuperar y reutilizar nuestra solución ya calculada. Este enfoque se llama memorización (no memorización, aunque este término también se ajusta). Si estamos seguros de que ya no necesitaremos una solución particular, podemos tirarla para ahorrar espacio. En algunos casos, incluso podemos calcular las soluciones a subproblemas que sabemos que necesitaremos de antemano.